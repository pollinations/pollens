{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/notebooks/2%20Text-To-Video/1%20CLIP-Guided%20VQGAN%20-%20Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "![Ernst Haeckel Jellyfish](https://ipfs.pollinations.ai/ipfs/QmYK5P5zusrfVSv4SZXRXjD3uT344dUsbxJCiXq164oczn)\n",
        "\n",
        "*jellyfish by ernst haeckel* with a video of flames\n",
        "\n",
        "Upload a video, edit the result frame by frame. As with the image model, mentioning an artist or art style works well.\n",
        "\n",
        "--- \n",
        "\n",
        "It is possible to provide multiple text prompts with weights by providing them like this: \n",
        "**```oil painting: 0.5|salvador dali: 0.3|edward much:0.6|robot friend: 1.0|text:-0.5```** *In this case a negative weight next to `text` makes the model avoid text.*  \n",
        "\n",
        "Credits: [Katherine Crowson](https://twitter.com/RiversHaveWings), [jbuster](https://twitter.com/jbusted1), [thomash](https://twitter.com/pollinations_ai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pZmCmyKM9fmv",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Text Prompt\n",
        "text_input = 'jellyfish by magritte'  #@param {type: \"string\"}\n",
        "\n",
        "# Video prompt. Required!\n",
        "video_file = '' #@param {type: \"string\"}\n",
        "\n",
        "# FPS to extract video at\n",
        "fps =  12 #@param {type: \"number\"}\n",
        "\n",
        "# Width in pixels of image to be generated. If you put this too high the GPU may run out of memory. Rather use super-resolution to achieve high resolutions.\n",
        "width = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Height in pixels of image to be generated. (see width)\n",
        "height = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Apply a (neural) super-resolution step (2 x image width x image height)\n",
        "super_resolution = False #@param {type: \"boolean\"}\n",
        "\n",
        "# Frame Coherence (how strong to enforce temporal coherence between video frames / less flickering)\n",
        "frame_coherence =  0.65 #@param {type: \"number\"}\n",
        "\n",
        "# Step size (how fast to try and optimize the image. Between 0 and 100. the larger this value the further it will deviate from the input)\n",
        "step_size =  40 #@param {type: \"number\"}\n",
        "\n",
        "# Iterations (for how long to optimize each frame. the larger this value the further it will deviate from the input)\n",
        "iterations =  15 #@param {type: \"number\"}\n",
        "\n",
        "# Save intermediate frames during optimization (creates multiple versions of the video at different stages of optimization)\n",
        "save_intermediate_frames = False #@param {type: \"boolean\"}\n",
        "\n",
        "# Random seed. Each number will give a unique outpiut\n",
        "random_seed = 666 #@param {type: \"number\"}\n",
        "\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "social = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4LgXcz0UKvt"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "if not os.path.exists(video_file):\n",
        "  raise Exception(\"This model requires a video file to process. Please provide one as the input.\")\n",
        "\n",
        "!rm -rfv /content/frames\n",
        "!mkdir -p /content/frames\n",
        "!ffmpeg -i \"{video_file}\" -r {fps} /content/frames/frame%4d.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CUtyJQ0Ppv3"
      },
      "outputs": [],
      "source": [
        "# Model to use. ruDALLE was recently finetuned by a russian company called Sber and could give better outputs.\n",
        "imagemodel = \"imagenet\" \n",
        "#####@param ['imagenet', 'ruDALLE']\n",
        "\n",
        "\n",
        "# check if christmas and add christmas to prompt\n",
        "from datetime import datetime\n",
        "\n",
        "d = datetime.now()\n",
        "\n",
        "if social and (d.strftime(\"%m\") == \"12\") and  ((d.strftime(\"%d\") == \"24\") or  (d.strftime(\"%d\") == \"25\") or  (d.strftime(\"%d\") == \"26\")):\n",
        "    text_input = text_input + \"|christmas:3|text:-0.5\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VIf9Z1CTsC7R"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "#@title Upscale images/video frames\n",
        "\n",
        "black_and_white = False\n",
        "\n",
        "loaded_upscale_model = False\n",
        "\n",
        "def upscale(filepath):\n",
        "  if not super_resolution:\n",
        "    return\n",
        "  global loaded_upscale_model\n",
        "  if not loaded_upscale_model:\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd /content/Real-ESRGAN\n",
        "    !git checkout 3338b31f486586bd7f6b20cc2a9fadd5ed192a00\n",
        "    # Set up the environment\n",
        "    !pip install git+https://github.com/xinntao/BasicSR.git\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    %cd -\n",
        "    loaded_upscale_model = True \n",
        "  \n",
        "  %cd /content/Real-ESRGAN\n",
        "  !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth --input $filepath --netscale 4 --outscale 2 --half --output $output_path\n",
        "  filepath_out = filepath.replace(\".jpg\",\"_out.jpg\")\n",
        "  !mv -v $filepath_out $filepath\n",
        "  %cd -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf54A275YLpt"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSfISAhyPmyp"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $output_path\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "#%cd taming-transformers\n",
        "#!git checkout 2908a53b88478e5812d619b6ac003dbb29b069a0\n",
        "#%cd -\n",
        "#!pip install torch==1.10.0 torchvision@https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl torchaudio torchtext\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning==1.7.7\n",
        "!pip install kornia\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhhdWrSxQhwg"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "!sudo apt install -y aria2\n",
        "\n",
        "model_mapping = {\n",
        "    \"ruDALLE\": { \n",
        "        \"name\": \"vqgan_openimages_f16_8192.ckpt\",\n",
        "        \"config\": \"https://ipfs.pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt.yaml\",\n",
        "        \"checkpoint\": \"https://ipfs.pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt\",\n",
        "    },\n",
        "    \"imagenet\": {\n",
        "        \"name\": \"vqgan_imagenet_f16_16384.ckpt\",\n",
        "        \"config\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n",
        "        \"checkpoint\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1',        \n",
        "    } \n",
        "}\n",
        "\n",
        "selected_vqgan = model_mapping[imagemodel]\n",
        "vqgan_name = selected_vqgan[\"name\"]\n",
        "\n",
        "config = selected_vqgan[\"config\"]\n",
        "checkpoint = selected_vqgan[\"checkpoint\"]\n",
        "\n",
        "if not Path(vqgan_name).exists():\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{config}' -o {vqgan_name}.yaml\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{checkpoint}'  -o {vqgan_name}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "import tensorflow\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from taming.models import cond_transformer, vqgan\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import kornia.augmentation as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8wlve7GbR4D"
      },
      "outputs": [],
      "source": [
        "\n",
        "def noise_gen(shape):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    for i in reversed(range(5)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    try:\n",
        "        return vals[0], float(vals[1]), float(vals[2])\n",
        "    except:\n",
        "        return prompt, 1, float('-inf')\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvnTBhPGT1gn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLw9p5Rzacso"
      },
      "outputs": [],
      "source": [
        "step_size = step_size / 100\n",
        "step_size = step_size*step_size\n",
        "if vqgan_name == 'vqgan_openimages_f16_8192.ckpt':\n",
        "  step_size=step_size / 20\n",
        "\n",
        "z_orig = None\n",
        "mse_weight = None\n",
        "vid_index = 0\n",
        "previous_image = None\n",
        "previous_z = None\n",
        "def process_frame(image_prompt):\n",
        "  # scale step size\n",
        "  global z_orig, mse_weight, previous_z, vid_index\n",
        "    \n",
        "  args = argparse.Namespace(\n",
        "      \n",
        "      prompts=[t.strip() for t in text_input.split(\"|\")],\n",
        "      size=[width, height], \n",
        "      init_image=image_prompt,\n",
        "      init_weight= 0.5,\n",
        "\n",
        "      previous_image=previous_image,\n",
        "\n",
        "      # clip model settings\n",
        "      clip_model='ViT-B/32',\n",
        "      vqgan_config=f'{vqgan_name}.yaml',         \n",
        "      vqgan_checkpoint=vqgan_name,\n",
        "      step_size=step_size,\n",
        "      \n",
        "      # cutouts / crops\n",
        "      cutn=32,\n",
        "      cut_pow=1,\n",
        "      cut_size=224,\n",
        "\n",
        "      # display\n",
        "      display_freq=5,\n",
        "      seed=random_seed,\n",
        "      use_augs = True,\n",
        "      noise_fac= 0.1,\n",
        "\n",
        "      record_generation=True,\n",
        "\n",
        "      # noise and other constraints\n",
        "      use_noise = None,\n",
        "      constraint_regions = False,#\n",
        "      \n",
        "      \n",
        "      # add noise to embedding\n",
        "      noise_prompt_weights = None,\n",
        "      noise_prompt_seeds = [14575],#\n",
        "\n",
        "      # mse settings\n",
        "      mse_withzeros = True,\n",
        "      mse_decay_rate = 50,\n",
        "      mse_epoches = 10,\n",
        "\n",
        "      # end itteration\n",
        "      max_itter = iterations,\n",
        "  )\n",
        "\n",
        "\n",
        "  class MakeCutouts(nn.Module):\n",
        "      def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "          super().__init__()\n",
        "          self.cut_size = cut_size\n",
        "          self.cutn = cutn\n",
        "          self.cut_pow = cut_pow\n",
        "\n",
        "          self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "          self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "      def set_cut_pow(self, cut_pow):\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "      def forward(self, input):\n",
        "          sideY, sideX = input.shape[2:4]\n",
        "          max_size = min(sideX, sideY)\n",
        "          min_size = min(sideX, sideY, self.cut_size)\n",
        "          cutouts = []\n",
        "          cutouts_full = []\n",
        "          \n",
        "          min_size_width = min(sideX, sideY)\n",
        "          lower_bound = float(self.cut_size/min_size_width)\n",
        "          \n",
        "          for ii in range(self.cutn):\n",
        "              \n",
        "              \n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "          \n",
        "          cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "          if args.use_augs:\n",
        "            cutouts = augs(cutouts)\n",
        "\n",
        "          if args.noise_fac:\n",
        "            facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "            cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "          \n",
        "\n",
        "          return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "  %mkdir /content/vids\n",
        "\n",
        "  mse_decay = 0\n",
        "  if args.init_weight:\n",
        "    mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "  # <AUGMENTATIONS>\n",
        "  augs = nn.Sequential(\n",
        "      \n",
        "      K.RandomHorizontalFlip(p=0.5),\n",
        "      K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "      K.RandomPerspective(0.2,p=0.4, ),\n",
        "      K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "      )\n",
        "\n",
        "  noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "  image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "  image.save('init3.png')\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  print('Using device:', device)\n",
        "  print('using prompts: ', args.prompts)\n",
        "\n",
        "  tv_loss = TVLoss() \n",
        "\n",
        "  model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "  perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "  mse_weight = args.init_weight\n",
        "\n",
        "  cut_size = args.cut_size\n",
        "  # e_dim = model.quantize.e_dim\n",
        "\n",
        "  if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "      e_dim = 256\n",
        "      n_toks = model.quantize.n_embed\n",
        "      z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "      z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "  else:\n",
        "      e_dim = model.quantize.e_dim\n",
        "      n_toks = model.quantize.n_e\n",
        "      z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "      z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "  make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "  f = 2**(model.decoder.num_resolutions - 1)\n",
        "  toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "\n",
        "  if args.seed is not None:\n",
        "      torch.manual_seed(args.seed)\n",
        "\n",
        "  if args.init_image:\n",
        "      pil_image = Image.open(args.init_image).convert('RGB')\n",
        "      pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "      pil_image = TF.to_tensor(pil_image)\n",
        "      if args.use_noise:\n",
        "        pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "      z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "  if args.previous_image:\n",
        "      previous_pil_image = Image.open(args.previous_image).convert('RGB')\n",
        "      previous_pil_image = previous_pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "      previous_pil_image = TF.to_tensor(previous_pil_image)\n",
        "      previous_z, *_ = model.encode(previous_pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "\n",
        "  else:\n",
        "      \n",
        "      one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "          z = one_hot @ model.quantize.embed.weight\n",
        "      else:\n",
        "          z = one_hot @ model.quantize.embedding.weight\n",
        "      z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "  if args.mse_withzeros and not args.init_image:\n",
        "    z_orig = torch.zeros_like(z)\n",
        "  else:\n",
        "    z_orig = z.clone()\n",
        "\n",
        "  z.requires_grad = True\n",
        "\n",
        "  opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  pMs = []\n",
        "\n",
        "  if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "      gen = torch.Generator().manual_seed(seed)\n",
        "      embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "      pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "  for prompt in args.prompts:\n",
        "      txt, weight, stop = parse_prompt(prompt)\n",
        "      embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "  def synth_gumbel(z, quantize=True, saturate=True):\n",
        "      logits = model.quantize.proj(z)\n",
        "      if quantize:\n",
        "          one_hot = F.gumbel_softmax(logits, tau=1, hard=True, dim=1)\n",
        "      else:\n",
        "          one_hot = F.one_hot(logits.argmax(1), logits.shape[1]).movedim(3, 1).to(logits.dtype)\n",
        "      z_q = torch.einsum('nchw,cd->ndhw', one_hot, model.quantize.embed.weight)\n",
        "      return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "  def synth(z, quantize=True, saturate=True):\n",
        "      out = None\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        out = synth_gumbel(z, quantize, saturate)\n",
        "      else:\n",
        "        if args.constraint_regions:\n",
        "          z = replace_grad(z, z * z_mask)\n",
        "\n",
        "        if quantize:\n",
        "          z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "        else:\n",
        "          z_q = z.model\n",
        "\n",
        "        out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "      if saturate and not image_prompt:\n",
        "        progress = i / args.max_itter\n",
        "        saturation = max(0,min(1,(progress - 0.25) * 2))\n",
        "        out = transforms.functional.adjust_saturation(out, saturation)\n",
        "      \n",
        "      return out\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def checkin(i, losses):\n",
        "      losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "      tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "      out = synth(z, True)# False)\n",
        "\n",
        "      TF.to_pil_image(out[0].cpu()).save(f'progress.png')   \n",
        "      #display.display(display.Image(args.init_image))\n",
        "      #display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "  def ascend_txt(i):\n",
        "      global mse_weight\n",
        "      global previous_image\n",
        "      out = synth(z)\n",
        "      if args.record_generation:\n",
        "        with torch.no_grad():\n",
        "          global vid_index\n",
        "          out_a = synth(z, True)#, False)\n",
        "          if (save_intermediate_frames and i % 5 == 0) or i == args.max_itter -1:\n",
        "            filename = f'{output_path}/progress_{i:04}_{vid_index:05}.jpg'\n",
        "            TF.to_pil_image(out_a[0].cpu()).save(filename)\n",
        "            upscale(filename)\n",
        "            previous_image = filename\n",
        "\n",
        "      cutouts = make_cutouts(out)\n",
        "      cutouts = resample(cutouts, (perceptor.visual.input_resolution, perceptor.visual.input_resolution))\n",
        "\n",
        "\n",
        "      iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "      result = []\n",
        "\n",
        "      if args.init_weight:\n",
        "          \n",
        "          global z_orig, previous_z\n",
        "          \n",
        "          result.append(F.mse_loss(z, z_orig) * mse_weight / 2)\n",
        "          if previous_z is not None:\n",
        "            result.append(F.mse_loss(z, previous_z) * frame_coherence)\n",
        "          # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "          with torch.no_grad():\n",
        "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "              if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "                mse_weight = mse_weight - mse_decay\n",
        "                print(f\"updated mse weight: {mse_weight}\")\n",
        "              else:\n",
        "                mse_weight = 0\n",
        "                print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "      for prompt in pMs:\n",
        "          result.append(prompt(iii))\n",
        "\n",
        "      return result\n",
        "\n",
        "  #vid_index = 0\n",
        "  def train(i):\n",
        "      \n",
        "      opt.zero_grad()\n",
        "      lossAll = ascend_txt(i)\n",
        "\n",
        "      if i % args.display_freq == 0:\n",
        "          checkin(i, lossAll)\n",
        "      \n",
        "      loss = sum(lossAll)\n",
        "\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "  i = 0\n",
        "  try:\n",
        "      with tqdm() as pbar:\n",
        "          while True and i != args.max_itter:\n",
        "              train(i)\n",
        "\n",
        "              if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "                \n",
        "                opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "for image in tqdm(sorted(glob(\"/content/frames/*.jpg\"))):\n",
        "  print(\"processing video frame\", image)\n",
        "  vid_index = vid_index + 1\n",
        "  process_frame(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# create video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT3hKb5gJUPq"
      },
      "outputs": [],
      "source": [
        "out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "!mkdir -p /tmp/ffmpeg\n",
        "!cp $output_path/*.jpg /tmp/ffmpeg\n",
        "last_frame=!ls -t /tmp/ffmpeg/*.jpg | head -1\n",
        "last_frame = last_frame[0]\n",
        "\n",
        "# Copy last frame to start and duplicate at end so it sticks around longer\n",
        "end_still_seconds = 4\n",
        "!cp -v $last_frame /tmp/ffmpeg/0000.jpg\n",
        "#for i in range(end_still_seconds * 10):\n",
        "#  pad_file = f\"/tmp/ffmpeg/zzzz_pad_{i:05}.jpg\"\n",
        "#  !cp -v $last_frame $pad_file\n",
        "\n",
        "\n",
        "encoding_options = \"-c:v libx264 -crf 20 -preset slow -vf format=yuv420p -c:a aac -movflags +faststart\"\n",
        "\n",
        "!ffmpeg  -r {fps} -i /tmp/ffmpeg/%*.jpg -y {encoding_options} /tmp/vid_no_audio.mp4\n",
        "\n",
        "\n",
        "# Check if original video contains audio stream\n",
        "has_audio_stream = !ffprobe -v error -select_streams a -show_entries stream=codec_name -of default=noprint_wrappers=1:nokey=1 \"{video_file}\"\n",
        "print(\"has_audio_stream\",has_audio_stream)\n",
        "has_audio_stream = len(has_audio_stream) > 0\n",
        "\n",
        "if has_audio_stream:\n",
        "    !ffmpeg -i /tmp/vid_no_audio.mp4 -i \"{video_file}\"  -c copy -map 0:v:0 -map 1:a:0 -y \"{out_file}\"\n",
        "else:\n",
        "    !ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"{out_file}\"\n",
        "\n",
        "\n",
        "print(\"Written\", out_file)\n",
        "!sleep 2\n",
        "!rm -r /tmp/ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8qflaosdTVz"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "if not os.path.exists(out_file):\n",
        "  raise Exception(\"Expected output file does not exist.\")\n",
        "\n",
        "!sleep 50"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Text-To-Video - CLIP-Guided VQGAN (Video)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
